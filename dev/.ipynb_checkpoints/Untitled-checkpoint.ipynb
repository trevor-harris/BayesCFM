{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "cb5a851f-c4e1-4aca-8de6-f6b78c330294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class VelocityField(nn.Module):\n",
    "    def __init__(self, x_dim, cond_dim, hidden_dim=128, time_embed_dim=16):\n",
    "        super().__init__()\n",
    "        self.time_embed_dim = time_embed_dim\n",
    "        self.freqs = nn.Parameter(torch.linspace(1.0, 10.0, time_embed_dim), requires_grad=False)\n",
    "\n",
    "        input_dim = x_dim + time_embed_dim + cond_dim\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, x_dim)\n",
    "        )\n",
    "\n",
    "    def time_embed(self, t):\n",
    "        return torch.sin(t * self.freqs)\n",
    "\n",
    "    def forward(self, x_t, t, y):\n",
    "        \"\"\"\n",
    "        x_t: [B, d]  -- interpolated state\n",
    "        t:   [B, 1]  -- scalar time\n",
    "        y:   [B, c]  -- conditional input\n",
    "        \"\"\"\n",
    "        t_embed = self.time_embed(t)  # [B, time_embed_dim]\n",
    "        h = torch.cat([x_t, t_embed, y], dim=-1)\n",
    "        return self.net(h)  # [B, d]\n",
    "\n",
    "def flow_matching_loss(model, x0, x1, y):\n",
    "    \"\"\"\n",
    "    x0, x1: [B, d]   -- samples from conditional marginal\n",
    "    y:     [B, c]   -- conditioning variable\n",
    "    \"\"\"\n",
    "    B, d = x0.shape\n",
    "    t = torch.rand(B, 1, device=x0.device)\n",
    "    x_t = (1 - t) * x0 + t * x1\n",
    "    v_target = x1 - x0\n",
    "\n",
    "    v_pred = model(x_t, t, y)\n",
    "    return F.mse_loss(v_pred, v_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "7c016ae6-deab-4a52-bbdf-41f360100e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rk4_step(f, x, t, dt, y):\n",
    "    \"\"\"Performs one RK4 step for dx/dt = f(t, x) with conditioning y\"\"\"\n",
    "    k1 = f(t,     x,     y)\n",
    "    k2 = f(t+dt/2, x + dt/2 * k1, y)\n",
    "    k3 = f(t+dt/2, x + dt/2 * k2, y)\n",
    "    k4 = f(t+dt,   x + dt   * k3, y)\n",
    "    return x + (dt/6)*(k1 + 2*k2 + 2*k3 + k4)\n",
    "\n",
    "def rk4_integrate(f, x0, t0, t1, y, steps=100):\n",
    "    \"\"\"\n",
    "    Integrate dx/dt = f(t, x, y) from t0 to t1 using RK4\n",
    "\n",
    "    Args:\n",
    "        f: function f(t, x, y) -> dx/dt\n",
    "        x0: [B, d] initial state\n",
    "        t0: float\n",
    "        t1: float\n",
    "        y: [B, cond_dim] conditioning\n",
    "        steps: number of steps to integrate\n",
    "\n",
    "    Returns:\n",
    "        x_T: final state at t1\n",
    "    \"\"\"\n",
    "    dt = (t1 - t0) / steps\n",
    "    x = x0\n",
    "    t = t0\n",
    "    for _ in range(steps):\n",
    "        x = rk4_step(f, x, t, dt, y)\n",
    "        t += dt\n",
    "    return x\n",
    "\n",
    "def velocity_wrapper(model):\n",
    "    \"\"\"\n",
    "    Returns a callable f(t, x, y) to integrate with RK4\n",
    "    \"\"\"\n",
    "    def f(t, x, y):\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "        B = x.shape[0]\n",
    "        t_tensor = torch.ones(B, 1, device=x.device) * t\n",
    "        return model(x, t_tensor, y)\n",
    "    return f\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_from_flow(model, x0, y, t0=0.0, t1=1.0, steps=100):\n",
    "    f = velocity_wrapper(model)\n",
    "    return rk4_integrate(f, x0, t0, t1, y, steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24d5d87-fb22-4dbc-976f-13c902781ce9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "e9edba21-2272-4407-b578-84db50c2c78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvVelocityField(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels=64, time_embed_dim=16):\n",
    "        super().__init__()\n",
    "        self.time_embed_dim = time_embed_dim\n",
    "        self.freqs = nn.Parameter(torch.linspace(1.0, 10.0, time_embed_dim), requires_grad=False)\n",
    "\n",
    "        # Project time embedding to match spatial resolution\n",
    "        self.time_proj = nn.Linear(time_embed_dim, hidden_channels)\n",
    "\n",
    "        # Conv net\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels + hidden_channels, hidden_channels, kernel_size=3, padding=1),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(hidden_channels, hidden_channels, kernel_size=3, padding=1),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(hidden_channels, hidden_channels, kernel_size=3, padding=1),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(hidden_channels, hidden_channels, kernel_size=3, padding=1),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(hidden_channels, hidden_channels, kernel_size=3, padding=1),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(hidden_channels, hidden_channels, kernel_size=3, padding=1),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(hidden_channels, hidden_channels, kernel_size=3, padding=1),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(hidden_channels, in_channels, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "    def time_embed(self, t):\n",
    "        # t: [B, 1]\n",
    "        return torch.sin(t * self.freqs)  # [B, time_embed_dim]\n",
    "\n",
    "    def forward(self, x_t, t):\n",
    "        \"\"\"\n",
    "        x_t: [B, C, H, W]  -- interpolated state\n",
    "        t:   [B, 1]        -- scalar time\n",
    "        \"\"\"\n",
    "        # Create time embedding\n",
    "        t_embed = self.time_embed(t)  # [B, time_embed_dim]\n",
    "        t_proj = self.time_proj(t_embed)  # [B, hidden_channels]\n",
    "        # Broadcast to spatial dims\n",
    "        t_proj = t_proj[:, :, None, None].expand(-1, -1, x_t.shape[2], x_t.shape[3])\n",
    "\n",
    "        # Concatenate time conditioning\n",
    "        h = torch.cat([x_t, t_proj], dim=1)  # [B, C+hidden_channels, H, W]\n",
    "        return self.net(h)  # [B, C, H, W]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "6ddc56ef-71f7-484a-876a-a795995da162",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Small helpers: time embedding and residual block that accepts time\n",
    "# ---------------------------------------------------------------------\n",
    "class TimeEmbed(nn.Module):\n",
    "    def __init__(self, time_embed_dim=16, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        # fixed freqs as in your original code (not learned)\n",
    "        self.freqs = nn.Parameter(torch.linspace(1.0, 10.0, time_embed_dim), requires_grad=False)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(time_embed_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, t):\n",
    "        # t: [B, 1]\n",
    "        # produces [B, hidden_dim]\n",
    "        x = torch.sin(t * self.freqs)  # [B, time_embed_dim]\n",
    "        return self.mlp(x)\n",
    "\n",
    "\n",
    "class ResConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual conv block which adds a (broadcasted) time embedding after first conv.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch, time_dim, groups=8):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1)\n",
    "        self.gn1 = nn.GroupNorm(groups, out_ch)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1)\n",
    "        self.gn2 = nn.GroupNorm(groups, out_ch)\n",
    "        self.time_proj = nn.Linear(time_dim, out_ch)\n",
    "\n",
    "        # if channel mismatch, use 1x1 to match for residual\n",
    "        self.nin_shortcut = nn.Identity()\n",
    "        if in_ch != out_ch:\n",
    "            self.nin_shortcut = nn.Conv2d(in_ch, out_ch, kernel_size=1)\n",
    "\n",
    "    def forward(self, x, t_emb):\n",
    "        \"\"\"\n",
    "        x: [B, C, H, W]\n",
    "        t_emb: [B, time_dim]\n",
    "        \"\"\"\n",
    "        h = self.conv1(x)\n",
    "        # add time embedding (project & broadcast)\n",
    "        t = self.time_proj(t_emb)[:, :, None, None]\n",
    "        h = self.gn1(h + t)\n",
    "        h = F.silu(h)\n",
    "\n",
    "        h = self.conv2(h)\n",
    "        h = self.gn2(h)\n",
    "        h = F.silu(h)\n",
    "\n",
    "        return h + self.nin_shortcut(x)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# U-Net velocity field\n",
    "# ---------------------------------------------------------------------\n",
    "class ConvUNetVelocityField(nn.Module):\n",
    "    def __init__(self, in_channels=1, base_channels=64, time_embed_dim=16, time_mlp_dim=128, depths=(1, 1, 2)):\n",
    "        \"\"\"\n",
    "        - in_channels: image channels (1 for MNIST, 3 for CIFAR)\n",
    "        - base_channels: number of channels in the first stage\n",
    "        - time_embed_dim: size of the sinusoidal frequency embedding\n",
    "        - time_mlp_dim: hidden dim for time MLP (injected into ResConvBlock)\n",
    "        - depths: tuple controlling number of ResConvBlocks per stage (down / bottleneck / up)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.time_mlp = TimeEmbed(time_embed_dim=time_embed_dim, hidden_dim=time_mlp_dim)\n",
    "\n",
    "        # Encoder stages\n",
    "        self.enc_blocks = nn.ModuleList()\n",
    "        self.downs = nn.ModuleList()\n",
    "        in_ch = in_channels\n",
    "        channels = []\n",
    "        for i, d in enumerate(depths):\n",
    "            out_ch = base_channels * (2 ** i)\n",
    "            for _ in range(d):\n",
    "                self.enc_blocks.append(ResConvBlock(in_ch, out_ch, time_dim=time_mlp_dim))\n",
    "                in_ch = out_ch\n",
    "            channels.append(out_ch)\n",
    "            # downsample between stages (except after last)\n",
    "            self.downs.append(nn.Conv2d(out_ch, out_ch, kernel_size=4, stride=2, padding=1))  # halves H,W\n",
    "\n",
    "        # Bottleneck (an extra block)\n",
    "        self.bot_block = ResConvBlock(in_ch, in_ch * 2, time_dim=time_mlp_dim)\n",
    "        bot_ch = in_ch * 2\n",
    "\n",
    "        # Decoder stages (mirror)\n",
    "        self.up_convs = nn.ModuleList()\n",
    "        self.dec_blocks = nn.ModuleList()\n",
    "        rev_channels = list(reversed(channels))\n",
    "        in_ch = bot_ch\n",
    "        for i, out_ch in enumerate(rev_channels):\n",
    "            # upsample via ConvTranspose2d to double spatial dims\n",
    "            self.up_convs.append(nn.ConvTranspose2d(in_ch, out_ch, kernel_size=4, stride=2, padding=1))\n",
    "            # After concatenation with skip, process with ResConvBlock(s)\n",
    "            self.dec_blocks.append(ResConvBlock(in_ch=out_ch * 2, out_ch=out_ch, time_dim=time_mlp_dim))\n",
    "            in_ch = out_ch\n",
    "\n",
    "        # final conv to map back to image channels\n",
    "        self.final_conv = nn.Sequential(\n",
    "            nn.Conv2d(base_channels, base_channels, kernel_size=3, padding=1),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(base_channels, in_channels, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_t, t):\n",
    "        \"\"\"\n",
    "        x_t: [B, C, H, W]\n",
    "        t:   [B, 1]  (scalar time per sample)\n",
    "        returns velocity of same shape as x_t: [B, C, H, W]\n",
    "        \"\"\"\n",
    "        B, C, H, W = x_t.shape\n",
    "        t_emb = self.time_mlp(t)  # [B, time_mlp_dim]\n",
    "\n",
    "        # Encoder: store skips\n",
    "        skips = []\n",
    "        idx = 0\n",
    "        h = x_t\n",
    "        # iterate over enc_blocks and downs; enc_blocks is flattened sequence\n",
    "        enc_block_iter = iter(self.enc_blocks)\n",
    "        for down in self.downs:\n",
    "            # each stage may have multiple blocks (depending on how many were appended)\n",
    "            # We find blocks until next down is consumed (we appended d blocks per stage)\n",
    "            # Implementation detail: we rely on the ordering used during creation\n",
    "            # Apply the number of blocks that correspond to this stage by applying blocks until shape changes from down op\n",
    "            # Simpler: apply one block per stage loop per depths value; earlier we stored \"depths\"\n",
    "            # But to keep logic simple and robust, we apply blocks until the next down is applied.\n",
    "            # Here we just apply one block per downs stage (matching how we built enc_blocks).\n",
    "            block = next(enc_block_iter)\n",
    "            h = block(h, t_emb)\n",
    "            skips.append(h)\n",
    "            h = down(h)\n",
    "\n",
    "        # There may be remaining enc_blocks if depths had >1 in a stage; consume them\n",
    "        for block in enc_block_iter:\n",
    "            h = block(h, t_emb)\n",
    "\n",
    "        # Bottleneck\n",
    "        h = self.bot_block(h, t_emb)\n",
    "\n",
    "        # Decoder: upsample and use skips\n",
    "        for up_conv, dec_block in zip(self.up_convs, self.dec_blocks):\n",
    "            h = up_conv(h)\n",
    "            skip = skips.pop()  # last skip\n",
    "            # If shapes differ by 1 due to odd sizes, center-crop skip to match\n",
    "            if skip.shape[-2:] != h.shape[-2:]:\n",
    "                # simple center crop\n",
    "                sh, sw = skip.shape[-2:]\n",
    "                hh, ww = h.shape[-2:]\n",
    "                start_h = (sh - hh) // 2\n",
    "                start_w = (sw - ww) // 2\n",
    "                skip = skip[..., start_h:start_h+hh, start_w:start_w+ww]\n",
    "            # concat channels\n",
    "            h = torch.cat([h, skip], dim=1)\n",
    "            h = dec_block(h, t_emb)\n",
    "\n",
    "        # final conv\n",
    "        v = self.final_conv(h)\n",
    "        v = nn.Upsample((28, 28))(v)\n",
    "        return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6dd43075-fbd1-4990-938c-8cd95d7f0c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # Mean and std for MNIST\n",
    "])\n",
    "\n",
    "# Download and load the training dataset\n",
    "train_dataset = datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Download and load the test dataset\n",
    "test_dataset = datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "# # Example: iterate over the training data\n",
    "# for batch_idx, (x, y) in enumerate(train_loader):\n",
    "#     print(f\"Batch {batch_idx} | Data shape: {x.shape} | Target shape: {y.shape}\")\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "20140482-bcb4-444a-a46d-1a8740729386",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvUNetVelocityField(in_channels=1, base_channels=64, time_embed_dim=16, time_mlp_dim=128, depths=(1,1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e436e4b9-b66d-4f40-a372-27d6ffe5d2fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.rand(64, 1)\n",
    "v = model(x, t)\n",
    "v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d580b8a6-c0a4-46f3-8d47-06b926f29d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: iterate over the training data\n",
    "model = ConvUNetVelocityField(in_channels=1, base_channels=64, time_embed_dim=16, time_mlp_dim=128, depths=(1,1,2))\n",
    "for x, y in train_loader:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "286aa478-53d6-4e94-8c56-f52b69f84d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_flow_matching_loss(model, x0, x1):\n",
    "    \"\"\"\n",
    "    x0, x1: [B, d]   -- samples from conditional marginal\n",
    "    y:     [B, c]   -- conditioning variable\n",
    "    \"\"\"\n",
    "    n, c, h, w = x0.shape\n",
    "    t = torch.rand(n, 1, device=x0.device)\n",
    "    t_conv = t[:,:,None,None]\n",
    "    x_t = (1 - t_conv) * x0 + t_conv * x1\n",
    "    v_target = x1 - x0\n",
    "    v_pred = model(x_t, t)\n",
    "    return F.mse_loss(v_pred, v_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "8bc72a65-fb82-4151-89f7-7285d29e0280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_field_penalty(v_func, x, t, num_samples=1, sym_weight=1.0, eig_weight=1.0):\n",
    "    \"\"\"\n",
    "    Computes a regularization penalty to encourage v ≈ ∇φ (gradient of convex potential)\n",
    "\n",
    "    v_func: velocity model, takes (x, t, y) -> [B, D]\n",
    "    x: [B, D], requires_grad=True\n",
    "    t: [B, 1]\n",
    "    y: [B, C]\n",
    "    Returns:\n",
    "        scalar penalty\n",
    "    \"\"\"\n",
    "    # B, D = x.shape\n",
    "    n, c, h, w = x.shape\n",
    "    device = x.device\n",
    "    penalty = 0.0\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        z = torch.randn_like(x)  # Hutchinson probe\n",
    "\n",
    "        # Vector-Jacobian product: J_v z\n",
    "        v = v_func(x, t)  # [B, D]\n",
    "        Jv_z = torch.autograd.grad(\n",
    "            outputs=v,\n",
    "            inputs=x,\n",
    "            grad_outputs=z,\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            only_inputs=True\n",
    "        )[0]  # [B, D]\n",
    "\n",
    "        # Jacobian-Vector product: J_v^T z = ∇_x (v^T z)\n",
    "        vz = torch.sum(v * z, dim=1)  # [B]\n",
    "        JvT_z = torch.autograd.grad(\n",
    "            outputs=vz,\n",
    "            inputs=x,\n",
    "            grad_outputs=torch.ones_like(vz),\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            only_inputs=True\n",
    "        )[0]  # [B, D]\n",
    "\n",
    "        # Symmetry penalty: || Jv_z - JvT_z ||^2\n",
    "        sym_penalty = ((Jv_z - JvT_z)**2).sum(dim=1)  # [B]\n",
    "\n",
    "        # Convexity (PSD) penalty: penalize z^T Jv z < 0\n",
    "        zT_Jv_z = (Jv_z * z).sum(dim=1)  # [B]\n",
    "        eig_penalty = F.relu(-zT_Jv_z)**2  # [B]\n",
    "\n",
    "        penalty += sym_weight * sym_penalty.mean() + eig_weight * eig_penalty.mean()\n",
    "\n",
    "    return penalty / num_samples\n",
    "\n",
    "def CGM_loss(model, x0, x1, reg_weight=1.0, sym_weight=1.0, eig_weight=1.0):\n",
    "    n, c, h, w = x0.shape\n",
    "    t = torch.rand(n, 1, device=x0.device)\n",
    "    t_conv = t[:,:,None,None]\n",
    "    x_t = (1 - t_conv) * x0 + t_conv * x1\n",
    "    x_t = x_t.detach().requires_grad_()\n",
    "    v_target = x1 - x0\n",
    "    v_pred = model(x_t, t)\n",
    "    \n",
    "    fm_loss = F.mse_loss(v_pred, v_target)\n",
    "    reg_penalty = gradient_field_penalty(model, x_t, t,sym_weight=sym_weight, eig_weight=eig_weight)\n",
    "\n",
    "    return fm_loss + reg_weight * reg_penalty, {\n",
    "        'fm_loss': fm_loss.item(),\n",
    "        'reg_penalty': reg_penalty.item()\n",
    "    }\n",
    "\n",
    "def add_sgmcmc_noise(model, lr, noise_scale=1.0):\n",
    "    for p in model.parameters():\n",
    "        if p.grad is None:\n",
    "            continue\n",
    "        noise = torch.randn_like(p) * torch.sqrt(torch.tensor(2.0 * lr)) * noise_scale\n",
    "        p.data.add_(noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "1e997af4-ac29-4a72-8965-a0e652b3eaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rk4_step(f, x, t, dt):\n",
    "    \"\"\"Performs one RK4 step for dx/dt = f(t, x) with conditioning y\"\"\"\n",
    "    k1 = f(t,     x)\n",
    "    k2 = f(t+dt/2, x + dt/2 * k1)\n",
    "    k3 = f(t+dt/2, x + dt/2 * k2)\n",
    "    k4 = f(t+dt,   x + dt   * k3)\n",
    "    return x + (dt/6)*(k1 + 2*k2 + 2*k3 + k4)\n",
    "\n",
    "def rk4_integrate(f, x0, t0, t1,steps=100):\n",
    "    \"\"\"\n",
    "    Integrate dx/dt = f(t, x, y) from t0 to t1 using RK4\n",
    "\n",
    "    Args:\n",
    "        f: function f(t, x, y) -> dx/dt\n",
    "        x0: [B, d] initial state\n",
    "        t0: float\n",
    "        t1: float\n",
    "        y: [B, cond_dim] conditioning\n",
    "        steps: number of steps to integrate\n",
    "\n",
    "    Returns:\n",
    "        x_T: final state at t1\n",
    "    \"\"\"\n",
    "    dt = (t1 - t0) / steps\n",
    "    x = x0\n",
    "    t = t0\n",
    "    for _ in range(steps):\n",
    "        x = rk4_step(f, x, t, dt)\n",
    "        t += dt\n",
    "    return x\n",
    "\n",
    "def velocity_wrapper(model):\n",
    "    \"\"\"\n",
    "    Returns a callable f(t, x, y) to integrate with RK4\n",
    "    \"\"\"\n",
    "    def f(t, x):\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "        B = x.shape[0]\n",
    "        t_tensor = torch.ones(B, 1, device=x.device) * t\n",
    "        return model(x, t_tensor)\n",
    "    return f\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_from_flow(model, x0, t0=0.0, t1=1.0, steps=100):\n",
    "    f = velocity_wrapper(model)\n",
    "    return rk4_integrate(f, x0, t0, t1, steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "df1195f1-8950-4728-ba27-2f06c8883a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conv_flow_matching_loss(model, torch.randn_like(x), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "6d0e453b-4467-40b1-a59a-6805cd5032ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "967b0faa-1f26-4660-a0eb-cbd144988bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "bd813a62-d830-4cc7-aa7c-4885e30576d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "129a8e15314145a7bae5a25c79c75dfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83f350c751ab4a6281c8398b02a4b6b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6665, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b191e9f4cb8b47fe900d328a5c35a1c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6441, device='cuda:0')\n",
      "tensor(0.5946, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0087c84ca970422083f4fb5854beccb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6087, device='cuda:0')\n",
      "tensor(0.6145, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99437c779de54fdb9e2341923dd0b90d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5943, device='cuda:0')\n",
      "tensor(0.6073, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81f549ac752848e48a9c191a12a7f5cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5881, device='cuda:0')\n",
      "tensor(0.5584, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad2607e74469489dbb4dd766c70fa999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5848, device='cuda:0')\n",
      "tensor(0.5803, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89c506bd52dc421fa066f811f280b9a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5481, device='cuda:0')\n",
      "tensor(0.5901, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2f0926059b84bc8bc3af13b21476d95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5564, device='cuda:0')\n",
      "tensor(0.5813, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7785af73104140f1926386e0caf61dc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5759, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a51b69a26f7b4d0fb8d74c99e1c177e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5780, device='cuda:0')\n",
      "tensor(0.5476, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-3\n",
    "model = ConvUNetVelocityField(in_channels=1, base_channels=64, time_embed_dim=16, time_mlp_dim=128, depths=(1,1,2))\n",
    "model = model.cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "k = 0\n",
    "for step in trange(10):\n",
    "    for x, y in tqdm(train_loader, leave = False):\n",
    "        x1 = x.cuda()\n",
    "        x0 = torch.randn_like(x1).cuda()\n",
    "        loss = conv_flow_matching_loss(model, x0, x1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        k += 1\n",
    "        if k % 500 == 0: \n",
    "            print(loss.detach())\n",
    "      # add_sgmcmc_noise(model, lr=lr, noise_scale=1e-3)k += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "3daf4613-728c-47be-9028-af7d88e22e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgmcmc_sample(model, lr, noise_scale=1.0):\n",
    "  import copy\n",
    "  model_samp = copy.deepcopy(model)\n",
    "  for p in model_samp.parameters():\n",
    "    noise = torch.randn_like(p) * torch.sqrt(torch.tensor(2.0 * lr)) * noise_scale\n",
    "    p.data.add_(noise)\n",
    "  return model_samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e184da1b-eeaa-4d76-b457-5e9a035cf760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr = 1e-3\n",
    "\n",
    "# model_samp = sgmcmc_sample(model, lr, noise_scale=1.0)\n",
    "# x1_samp1 = sample_from_flow(model_samp, torch.randn(1000, 1), 0*torch.randn(1000, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "c88dd8ea-575d-43f0-aa1d-f1d5ffbe8f9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x1_samp = sample_from_flow(model, torch.randn_like(x1).cuda())\n",
    "\n",
    "fig, ax = plt.subplots(1, 4, constrained_layout = True, figsize = (10, 4))\n",
    "ax[0].imshow(x1_samp[0,0].cpu(), cmap = 'binary')\n",
    "ax[1].imshow(x1_samp[1,0].cpu(), cmap = 'binary')\n",
    "ax[2].imshow(x1_samp[2,0].cpu(), cmap = 'binary')\n",
    "ax[3].imshow(x1_samp[3,0].cpu(), cmap = 'binary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "d00d362f-579f-435a-a6b2-cde54ad84d8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f03128a92e54969a7955a722d02b4b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea243c95e71a4b639e306b45d1aa360b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3902, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd9eacec245a4ef99ee5736dc6fb0547",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3480, device='cuda:0')\n",
      "tensor(0.3313, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a9e17c18c3345a0bba52125e77c2b88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3202, device='cuda:0')\n",
      "tensor(0.3403, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e48dd6054b4343ca84c1d77b31cdb8a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3126, device='cuda:0')\n",
      "tensor(0.3571, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f6ff69c79d04b8d906fdc0732e0448b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2889, device='cuda:0')\n",
      "tensor(0.2792, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3838220b4d0a4b43a63b15a7d474b792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3739, device='cuda:0')\n",
      "tensor(0.3253, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c782abeb380342f182956615e49873ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2921, device='cuda:0')\n",
      "tensor(0.2648, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2edda99025664cd6b7a530710fcd9696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3264, device='cuda:0')\n",
      "tensor(0.2874, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ae6ccb22f9b499da087807ed48734c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3321, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c0338e1714748c3a62f67ec9af3f764",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3225, device='cuda:0')\n",
      "tensor(0.3268, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "631542a6a9f340ca85dff98a687e9de2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2913, device='cuda:0')\n",
      "tensor(0.3229, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5673d36a4222452583a6624d13274449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3039, device='cuda:0')\n",
      "tensor(0.4272, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a68c300c06ec4a3295e5656cd947e52e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3166, device='cuda:0')\n",
      "tensor(0.3244, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "003b40ab32574eb4bb24f9c91cf8bcd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3129, device='cuda:0')\n",
      "tensor(0.3114, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3542631692874a2b8057a944fce479b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3299, device='cuda:0')\n",
      "tensor(1.9206, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4229fe6cfed45ccbec54bd9a27fdcf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.0195, device='cuda:0')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[189], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m conv_flow_matching_loss(model, x0, x1)\n\u001b[1;32m     12\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 13\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     16\u001b[0m k \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/climate/lib/python3.12/site-packages/torch/optim/optimizer.py:493\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    489\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    490\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 493\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/climate/lib/python3.12/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/climate/lib/python3.12/site-packages/torch/optim/adam.py:244\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    232\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    234\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    235\u001b[0m         group,\n\u001b[1;32m    236\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    241\u001b[0m         state_steps,\n\u001b[1;32m    242\u001b[0m     )\n\u001b[0;32m--> 244\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/climate/lib/python3.12/site-packages/torch/optim/optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/climate/lib/python3.12/site-packages/torch/optim/adam.py:876\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    874\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 876\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/climate/lib/python3.12/site-packages/torch/optim/adam.py:621\u001b[0m, in \u001b[0;36m_multi_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;66;03m# Use device beta1 if beta1 is a tensor to ensure all\u001b[39;00m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;66;03m# tensors are on the same device\u001b[39;00m\n\u001b[1;32m    619\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_lerp_(device_exp_avgs, device_grads, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m device_beta1)\n\u001b[0;32m--> 621\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_mul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;66;03m# Due to the strictness of the _foreach_addcmul API, we can't have a single\u001b[39;00m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;66;03m# tensor scalar as the scalar arg (only python number is supported there)\u001b[39;00m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;66;03m# as a result, separate out the value mul\u001b[39;00m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;66;03m# Filed https://github.com/pytorch/pytorch/issues/139795\u001b[39;00m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(beta2, torch\u001b[38;5;241m.\u001b[39mTensor):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr = 1e-3\n",
    "model = ConvVelocityField(in_channels=1, hidden_channels=128, time_embed_dim=64)\n",
    "model = model.cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "k = 0\n",
    "for step in trange(25):\n",
    "    for x, y in tqdm(train_loader, leave = False):\n",
    "        x1 = x.cuda()\n",
    "        x0 = torch.randn_like(x1).cuda()\n",
    "        loss = conv_flow_matching_loss(model, x0, x1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        k += 1\n",
    "        if k % 500 == 0: \n",
    "            print(loss.detach())\n",
    "      # add_sgmcmc_noise(model, lr=lr, noise_scale=1e-3)k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "0a426bde-bd03-4bd2-ac60-28009b89b2e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x1_samp = sample_from_flow(model, torch.randn_like(x1).cuda())\n",
    "\n",
    "fig, ax = plt.subplots(1, 4, constrained_layout = True, figsize = (10, 4))\n",
    "ax[0].imshow(x1_samp[0,0].cpu(), cmap = 'binary')\n",
    "ax[1].imshow(x1_samp[1,0].cpu(), cmap = 'binary')\n",
    "ax[2].imshow(x1_samp[2,0].cpu(), cmap = 'binary')\n",
    "ax[3].imshow(x1_samp[3,0].cpu(), cmap = 'binary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a0140e-065f-459b-8444-0ae69f102f5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "dbdfec61-90ef-4208-a985-41e4ed9818ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# -------------------------\n",
    "# Spectral conv (custom FNO)\n",
    "# -------------------------\n",
    "class SpectralConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, modes1, modes2):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.modes1 = modes1\n",
    "        self.modes2 = modes2\n",
    "\n",
    "        self.weight_real = nn.Parameter(\n",
    "            torch.randn(in_channels, out_channels, modes1, modes2) * 0.01\n",
    "        )\n",
    "        self.weight_imag = nn.Parameter(\n",
    "            torch.randn(in_channels, out_channels, modes1, modes2) * 0.01\n",
    "        )\n",
    "\n",
    "    def compl_mul2d(self, input_ft, w_real, w_imag):\n",
    "        weight = torch.complex(w_real, w_imag)\n",
    "        return torch.einsum(\"bixy,ioxy->boxy\", input_ft, weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x_ft = torch.fft.rfft2(x, norm=\"ortho\")\n",
    "        out_ft = torch.zeros(B, self.out_channels, H, W//2 + 1, dtype=torch.cfloat, device=x.device)\n",
    "\n",
    "        m1 = min(self.modes1, H)\n",
    "        m2 = min(self.modes2, W//2 + 1)\n",
    "        out_ft[:, :, :m1, :m2] = self.compl_mul2d(\n",
    "            x_ft[:, :, :m1, :m2],\n",
    "            self.weight_real[:, :, :m1, :m2],\n",
    "            self.weight_imag[:, :, :m1, :m2]\n",
    "        )\n",
    "\n",
    "        x_out = torch.fft.irfft2(out_ft, s=(H, W), norm=\"ortho\")\n",
    "        return x_out\n",
    "\n",
    "# -------------------------\n",
    "# Time embedding\n",
    "# -------------------------\n",
    "class TimeEmbed(nn.Module):\n",
    "    def __init__(self, time_embed_dim=16, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.freqs = nn.Parameter(torch.linspace(1.0, 10.0, time_embed_dim), requires_grad=False)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(time_embed_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, t):\n",
    "        x = torch.sin(t * self.freqs)\n",
    "        return self.mlp(x)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Simple FNO block\n",
    "# -------------------------\n",
    "class FNOBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, modes1, modes2, time_dim):\n",
    "        super().__init__()\n",
    "        self.spectral = SpectralConv2d(in_ch, out_ch, modes1, modes2)\n",
    "        self.pointwise = nn.Conv2d(in_ch, out_ch, kernel_size=1)\n",
    "        self.time_proj = nn.Linear(time_dim, out_ch)\n",
    "        self.norm = nn.GroupNorm(8, out_ch)\n",
    "        self.shortcut = nn.Identity() if in_ch == out_ch else nn.Conv2d(in_ch, out_ch, 1)\n",
    "\n",
    "    def forward(self, x, t_emb):\n",
    "        h = self.spectral(x) + self.pointwise(x)\n",
    "        h = h + self.time_proj(t_emb)[:, :, None, None]\n",
    "        h = self.norm(h)\n",
    "        h = F.silu(h)\n",
    "        return h + self.shortcut(x)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Velocity field with stacked FNO blocks\n",
    "# -------------------------\n",
    "class FNOVelocityField(nn.Module):\n",
    "    def __init__(self, in_channels=1, hidden_channels=64, time_embed_dim=16, modes1=12, modes2=12, num_layers=4):\n",
    "        super().__init__()\n",
    "        self.time_mlp = TimeEmbed(time_embed_dim=time_embed_dim, hidden_dim=hidden_channels)\n",
    "\n",
    "        layers = []\n",
    "        ch = in_channels\n",
    "        for i in range(num_layers):\n",
    "            layers.append(FNOBlock(ch, hidden_channels, modes1, modes2, hidden_channels))\n",
    "            ch = hidden_channels\n",
    "        self.fno_layers = nn.ModuleList(layers)\n",
    "\n",
    "        self.final = nn.Conv2d(hidden_channels, in_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x_t, t):\n",
    "        t_emb = self.time_mlp(t)\n",
    "        h = x_t\n",
    "        for layer in self.fno_layers:\n",
    "            h = layer(h, t_emb)\n",
    "        return self.final(h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "d73fd8cb-b252-48f5-87fc-d0e50df2f25e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb50a379faf74658b250c550726be0f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b56640de93b244a49cdf3c5cafc25f8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4019, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ee6f63f3eb141c7b98e26f50fc71911",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4892, device='cuda:0')\n",
      "tensor(0.4603, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d61c909a13e4795a6fa26d4de56065b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4269, device='cuda:0')\n",
      "tensor(0.3205, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0f1f8ed07fa41f5a9f3aec68d47fef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3829, device='cuda:0')\n",
      "tensor(0.3877, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "802affdd339f4d30b59037483a7372b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4721, device='cuda:0')\n",
      "tensor(0.4512, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58c2715217b34ac2afa222d21a063f44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3704, device='cuda:0')\n",
      "tensor(0.4142, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0297245ac8f24c518b8acbea9614e776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3626, device='cuda:0')\n",
      "tensor(0.3979, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59c695d36ee64aae955bfa37c43ac1d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4676, device='cuda:0')\n",
      "tensor(0.3072, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e1d05b97b424b22966b7d59f71d19ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4087, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c36aa09cc6bb490bbba8259f69485e13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3248, device='cuda:0')\n",
      "tensor(0.3534, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "948554dc3ce7439980cad21ef080e7e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3900, device='cuda:0')\n",
      "tensor(0.4015, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03073a2676bf40b2848d3b8848d4caff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3736, device='cuda:0')\n",
      "tensor(0.3261, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6933f378617840389a148a89ebba2bb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3983, device='cuda:0')\n",
      "tensor(0.3545, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8e72d7f954145b5bdce6cbcc3b1035e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3539, device='cuda:0')\n",
      "tensor(0.3833, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c26632498f142999897fe358de7600b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3847, device='cuda:0')\n",
      "tensor(0.3289, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-3\n",
    "model = FNOVelocityField(in_channels=1, modes1=16, modes2=16)\n",
    "model = model.cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "k = 0\n",
    "for step in trange(15):\n",
    "    for x, y in tqdm(train_loader, leave = False):\n",
    "        x1 = x.cuda()\n",
    "        x0 = torch.randn_like(x1).cuda()\n",
    "        loss = conv_flow_matching_loss(model, x0, x1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        k += 1\n",
    "        if k % 500 == 0: \n",
    "            print(loss.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "70aa1908-8075-4562-8b56-4007c18e3c32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x1_samp = sample_from_flow(model, torch.randn_like(x1).cuda())\n",
    "\n",
    "fig, ax = plt.subplots(1, 4, constrained_layout = True, figsize = (10, 4))\n",
    "ax[0].imshow(x1_samp[0,0].cpu(), cmap = 'binary')\n",
    "ax[1].imshow(x1_samp[1,0].cpu(), cmap = 'binary')\n",
    "ax[2].imshow(x1_samp[2,0].cpu(), cmap = 'binary')\n",
    "ax[3].imshow(x1_samp[3,0].cpu(), cmap = 'binary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d3bfd3-28fd-4c31-a73b-ec18a4c40170",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "dd586f00-881e-4743-93df-8216a0fefa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def flatten_image_batch(x):\n",
    "    B, C, H, W = x.shape\n",
    "    x_flat = x.view(B, -1)  # shape (B, D), D=C*H*W\n",
    "    return x_flat\n",
    "\n",
    "def unflatten_image_batch(x_flat, C, H, W):\n",
    "    B = x_flat.shape[0]\n",
    "    return x_flat.view(B, C, H, W)\n",
    "    \n",
    "def velocity_func_flat(x_flat, t, model, C, H, W):\n",
    "    x = unflatten_image_batch(x_flat, C, H, W)\n",
    "    v = model(x, t)  # velocity output, shape (B,C,H,W)\n",
    "    v_flat = flatten_image_batch(v)\n",
    "    return v_flat\n",
    "from torch.autograd.functional import jvp\n",
    "\n",
    "def jvp_velocity(x_flat, v_flat, t, model, C, H, W):\n",
    "    # Computes J(x) @ v_flat\n",
    "    def func(x_in):\n",
    "        return velocity_func_flat(x_in, t, model, C, H, W)\n",
    "    return jvp(func, (x_flat,), (v_flat,), create_graph=True)[1]\n",
    "\n",
    "def vjp_velocity(x_flat, v_flat, t, model, C, H, W):\n",
    "    # Computes J(x)^T @ v_flat\n",
    "    v_out = velocity_func_flat(x_flat, t, model, C, H, W)\n",
    "    grads = torch.autograd.grad(v_out, x_flat, grad_outputs=v_flat, create_graph=True, retain_graph=True)[0]\n",
    "    return grads\n",
    "\n",
    "def curl_reg(model, x_t, t, n_samples=1):\n",
    "    B, C, H, W = x_t.shape\n",
    "    x_flat = flatten_image_batch(x_t)\n",
    "    penalty = 0.\n",
    "    for _ in range(n_samples):\n",
    "        z = torch.randint(0, 2, x_flat.shape, device=x_t.device).float() * 2 - 1  # Rademacher vector\n",
    "        jvp_z = jvp_velocity(x_flat, z, t, model, C, H, W)\n",
    "        vjp_z = vjp_velocity(x_flat, z, t, model, C, H, W)\n",
    "        A_z = 0.5 * (jvp_z - vjp_z)\n",
    "        penalty += (A_z ** 2).sum(dim=-1).mean()  # mean over batch\n",
    "    return penalty / n_samples\n",
    "    \n",
    "def eigen_reg(model, x_t, t, n_samples=1):\n",
    "    B, C, H, W = x_t.shape\n",
    "    x_flat = flatten_image_batch(x_t)\n",
    "    penalty = 0.\n",
    "    for _ in range(n_samples):\n",
    "        z = torch.randn_like(x_flat, device=x_flat.device)\n",
    "        z = z / (z.norm(dim=-1, keepdim=True) + 1e-8)\n",
    "        jvp_z = jvp_velocity(x_flat, z, t, model, C, H, W)\n",
    "        rayleigh = (z * jvp_z).sum(dim=-1)\n",
    "        neg_part = F.relu(-rayleigh)\n",
    "        penalty += neg_part.mean()\n",
    "    return penalty / n_samples\n",
    "\n",
    "def cfm_loss(model, x_t, t, v_target):\n",
    "    v_pred = model(x_t, t)\n",
    "    return torch.mean((v_pred - v_target)**2)\n",
    "\n",
    "def gradient_flow_loss(model, x0, x1, lam_curl = 1e-3, lam_eig = 1e-3, n_samples = 1):\n",
    "    n, c, h, w = x0.shape\n",
    "    t = torch.rand(n, 1, device=x0.device)\n",
    "    t_conv = t[:,:,None,None]\n",
    "    x_t = (1 - t_conv) * x0 + t_conv * x1\n",
    "    x_t = x_t.detach().requires_grad_()\n",
    "    v_target = x1 - x0\n",
    "\n",
    "    loss_flow = cfm_loss(model, x_t, t, v_target)\n",
    "    loss_curl = curl_reg(model, x_t, t, n_samples=n_samples)\n",
    "    loss_eig = eigen_reg(model, x_t, t, n_samples=n_samples)\n",
    "\n",
    "    return loss_flow + lam_curl * loss_curl + lam_eig * loss_eig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "19f1522f-176e-429b-966a-7575909e7218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f895362897f946ddbcdae45fcf9e6833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d156362718b4c9eb18be6be30c42c9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4164, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8ac45bcc3cb4e76899681176662f0ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4576, device='cuda:0')\n",
      "tensor(0.4494, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d58bce3fe00c4129ae76dd5c5a08a422",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4613, device='cuda:0')\n",
      "tensor(0.3587, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5597754b17e4be18629b5386ffd2c1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4275, device='cuda:0')\n",
      "tensor(0.4414, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4755a0a239314faf8c2356eaca0b5c09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4768, device='cuda:0')\n",
      "tensor(0.4253, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b928aeca807a43609137ca2c39016ceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[358], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m x1 \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m      9\u001b[0m x0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn_like(x1)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m---> 10\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mgradient_flow_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[353], line 69\u001b[0m, in \u001b[0;36mgradient_flow_loss\u001b[0;34m(model, x0, x1, lam_curl, lam_eig, n_samples)\u001b[0m\n\u001b[1;32m     66\u001b[0m v_target \u001b[38;5;241m=\u001b[39m x1 \u001b[38;5;241m-\u001b[39m x0\n\u001b[1;32m     68\u001b[0m loss_flow \u001b[38;5;241m=\u001b[39m cfm_loss(model, x_t, t, v_target)\n\u001b[0;32m---> 69\u001b[0m loss_curl \u001b[38;5;241m=\u001b[39m \u001b[43mcurl_reg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m loss_eig \u001b[38;5;241m=\u001b[39m eigen_reg(model, x_t, t, n_samples\u001b[38;5;241m=\u001b[39mn_samples)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss_flow \u001b[38;5;241m+\u001b[39m lam_curl \u001b[38;5;241m*\u001b[39m loss_curl \u001b[38;5;241m+\u001b[39m lam_eig \u001b[38;5;241m*\u001b[39m loss_eig\n",
      "Cell \u001b[0;32mIn[353], line 37\u001b[0m, in \u001b[0;36mcurl_reg\u001b[0;34m(model, x_t, t, n_samples)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_samples):\n\u001b[1;32m     36\u001b[0m     z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, x_flat\u001b[38;5;241m.\u001b[39mshape, device\u001b[38;5;241m=\u001b[39mx_t\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mfloat() \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Rademacher vector\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m     jvp_z \u001b[38;5;241m=\u001b[39m \u001b[43mjvp_velocity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     vjp_z \u001b[38;5;241m=\u001b[39m vjp_velocity(x_flat, z, t, model, C, H, W)\n\u001b[1;32m     39\u001b[0m     A_z \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m (jvp_z \u001b[38;5;241m-\u001b[39m vjp_z)\n",
      "Cell \u001b[0;32mIn[353], line 23\u001b[0m, in \u001b[0;36mjvp_velocity\u001b[0;34m(x_flat, v_flat, t, model, C, H, W)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfunc\u001b[39m(x_in):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m velocity_func_flat(x_in, t, model, C, H, W)\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjvp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mv_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/climate/lib/python3.12/site-packages/torch/autograd/functional.py:448\u001b[0m, in \u001b[0;36mjvp\u001b[0;34m(func, inputs, v, create_graph, strict)\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;66;03m# The backward is linear so the value of grad_outputs is not important as\u001b[39;00m\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;66;03m# it won't appear in the double backward graph. We only need to ensure that\u001b[39;00m\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;66;03m# it does not contain inf or nan.\u001b[39;00m\n\u001b[1;32m    444\u001b[0m     grad_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m    445\u001b[0m         torch\u001b[38;5;241m.\u001b[39mzeros_like(out, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m out \u001b[38;5;129;01min\u001b[39;00m outputs\n\u001b[1;32m    446\u001b[0m     )\n\u001b[0;32m--> 448\u001b[0m     grad_inputs \u001b[38;5;241m=\u001b[39m \u001b[43m_autograd_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    449\u001b[0m     _check_requires_grad(grad_inputs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_inputs\u001b[39m\u001b[38;5;124m\"\u001b[39m, strict\u001b[38;5;241m=\u001b[39mstrict)\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m create_graph:\n",
      "File \u001b[0;32m~/climate/lib/python3.12/site-packages/torch/autograd/functional.py:195\u001b[0m, in \u001b[0;36m_autograd_grad\u001b[0;34m(outputs, inputs, grad_outputs, create_graph, retain_graph, is_grads_batched)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m,) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(inputs)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 195\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnew_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnew_grad_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_grads_batched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/climate/lib/python3.12/site-packages/torch/autograd/__init__.py:496\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    492\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[1;32m    493\u001b[0m         grad_outputs_\n\u001b[1;32m    494\u001b[0m     )\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    507\u001b[0m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[1;32m    508\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[1;32m    509\u001b[0m     ):\n",
      "File \u001b[0;32m~/climate/lib/python3.12/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr = 1e-3\n",
    "model = FNOVelocityField(in_channels=1, modes1=16, modes2=16)\n",
    "model = model.cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "for step in trange(15):\n",
    "    for x, y in tqdm(train_loader, leave = False):\n",
    "        x1 = x.cuda()\n",
    "        x0 = torch.randn_like(x1).cuda()\n",
    "        loss = gradient_flow_loss(model, x0, x1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        k += 1\n",
    "        if k % 500 == 0: \n",
    "            print(loss.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "7e2a5aac-c430-4e37-b142-7adf0705fe9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x1_samp = sample_from_flow(model, torch.randn_like(x1).cuda())\n",
    "\n",
    "fig, ax = plt.subplots(1, 4, constrained_layout = True, figsize = (10, 4))\n",
    "ax[0].imshow(x1_samp[0,0].cpu(), cmap = 'binary')\n",
    "ax[1].imshow(x1_samp[1,0].cpu(), cmap = 'binary')\n",
    "ax[2].imshow(x1_samp[2,0].cpu(), cmap = 'binary')\n",
    "ax[3].imshow(x1_samp[3,0].cpu(), cmap = 'binary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e036b6-fee4-44d1-9645-4b3ce7702d4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0996440a-f7a6-4d3b-b83a-7039f61685f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "4dd90cdf-8a45-4926-be82-902dc7d3e0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd.functional import jvp\n",
    "\n",
    "def jacobian_vector_product(func, x, v, create_graph=True):\n",
    "    # Returns J(x) @ v\n",
    "    y = func(x)\n",
    "    jvp_val = jvp(func, (x,), (v,), create_graph=create_graph)[1]\n",
    "    return jvp_val\n",
    "\n",
    "def vector_jacobian_product(func, x, v, create_graph=True):\n",
    "    # Returns v^T @ J(x) = J(x)^T @ v\n",
    "    y = func(x)\n",
    "    grads = torch.autograd.grad(y, x, grad_outputs=v, create_graph=create_graph, retain_graph=True)[0]\n",
    "    return grads\n",
    "\n",
    "def curl_penalty(func, x, n_samples=1):\n",
    "    d = x.shape[-1]\n",
    "    penalty = 0.\n",
    "    for _ in range(n_samples):\n",
    "        z = torch.randint(0, 2, x.shape, device=x.device).float() * 2 - 1  # Rademacher\n",
    "        jvp_z = jacobian_vector_product(func, x, z)\n",
    "        vjp_z = vector_jacobian_product(func, x, z)\n",
    "        A_z = 0.5 * (jvp_z - vjp_z)\n",
    "        penalty += (A_z**2).sum(dim=-1).mean()\n",
    "    return penalty / n_samples\n",
    "\n",
    "def eigenvalue_penalty(func, x, n_samples=1):\n",
    "    penalty = 0.\n",
    "    for _ in range(n_samples):\n",
    "        z = torch.randn_like(x)\n",
    "        z = z / (z.norm(dim=-1, keepdim=True) + 1e-8)\n",
    "        jvp_z = jacobian_vector_product(func, x, z)\n",
    "        rayleigh = (z * jvp_z).sum(dim=-1)  # batch dot product\n",
    "        neg_part = F.relu(-rayleigh)  # penalize negative part only\n",
    "        penalty += neg_part.mean()\n",
    "    return penalty / n_samples\n",
    "\n",
    "def ot_regularizer(func, x, n_samples=1, curl_wt=1.0, eig_wt=1.0):\n",
    "    curl = curl_penalty(func, x, n_samples)\n",
    "    eig = eigenvalue_penalty(func, x, n_samples)\n",
    "    return curl_wt * curl + eig_wt * eig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "6104e524-9da3-4509-b036-05240851f535",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "model = FNOVelocityField(in_channels=1, modes1=16, modes2=16)\n",
    "model = model.cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "7458a13c-a815-44c8-b631-d23f06b9269d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "FNOVelocityField.forward() missing 1 required positional argument: 't'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[304], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mot_regularizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[302], line 39\u001b[0m, in \u001b[0;36mot_regularizer\u001b[0;34m(func, x, n_samples, curl_wt, eig_wt)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mot_regularizer\u001b[39m(func, x, n_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, curl_wt\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, eig_wt\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m):\n\u001b[0;32m---> 39\u001b[0m     curl \u001b[38;5;241m=\u001b[39m \u001b[43mcurl_penalty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     eig \u001b[38;5;241m=\u001b[39m eigenvalue_penalty(func, x, n_samples)\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m curl_wt \u001b[38;5;241m*\u001b[39m curl \u001b[38;5;241m+\u001b[39m eig_wt \u001b[38;5;241m*\u001b[39m eig\n",
      "Cell \u001b[0;32mIn[302], line 21\u001b[0m, in \u001b[0;36mcurl_penalty\u001b[0;34m(func, x, n_samples)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_samples):\n\u001b[1;32m     20\u001b[0m     z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, x\u001b[38;5;241m.\u001b[39mshape, device\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mfloat() \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Rademacher\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m     jvp_z \u001b[38;5;241m=\u001b[39m \u001b[43mjacobian_vector_product\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     vjp_z \u001b[38;5;241m=\u001b[39m vector_jacobian_product(func, x, z)\n\u001b[1;32m     23\u001b[0m     A_z \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m (jvp_z \u001b[38;5;241m-\u001b[39m vjp_z)\n",
      "Cell \u001b[0;32mIn[302], line 6\u001b[0m, in \u001b[0;36mjacobian_vector_product\u001b[0;34m(func, x, v, create_graph)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mjacobian_vector_product\u001b[39m(func, x, v, create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Returns J(x) @ v\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     jvp_val \u001b[38;5;241m=\u001b[39m jvp(func, (x,), (v,), create_graph\u001b[38;5;241m=\u001b[39mcreate_graph)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jvp_val\n",
      "File \u001b[0;32m~/climate/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/climate/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[0;31mTypeError\u001b[0m: FNOVelocityField.forward() missing 1 required positional argument: 't'"
     ]
    }
   ],
   "source": [
    "ot_regularizer(model, x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c277f5-7f99-4c06-b6f0-6d33b730d133",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ca1deb-ac4e-4ac7-81ea-0f0b30d4f24a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918521e9-140b-4281-8065-70c5322fbb9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c5717a-7446-46df-9f0b-5302e8aa6eaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967bce34-d93b-4e81-9849-14c2ed137d69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9495e4-e9cd-4654-bf90-11d6ebec1ca3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "bc1fe0fd-59d3-4bd0-a1fc-a676698cf867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "\n",
    "class ConvICNNLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, use_hidden=True):\n",
    "        super().__init__()\n",
    "        # Convolution on input image with non-negative weights (softplus)\n",
    "        self.conv_in = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)\n",
    "        nn.init.uniform_(self.conv_in.weight, a=0, b=0.1)  # small positive init\n",
    "        \n",
    "        # Convolution on hidden state (previous layer output)\n",
    "        self.use_hidden = use_hidden\n",
    "        if use_hidden:\n",
    "            self.conv_hidden = nn.Conv2d(out_channels, out_channels, kernel_size=1, bias=True)\n",
    "            nn.init.zeros_(self.conv_hidden.bias)\n",
    "        \n",
    "        # FiLM scale and shift for conditioning + time\n",
    "        self.film_scale = nn.Linear(128, out_channels)  # assuming cond+time embed dim=128\n",
    "        self.film_shift = nn.Linear(128, out_channels)\n",
    "    \n",
    "    def forward(self, x_in, z_hidden, cond_emb):\n",
    "        # x_in: (B, C_in, H, W)\n",
    "        # z_hidden: (B, C_out, H, W) or None if first layer\n",
    "        # cond_emb: (B, 128) conditioning + time embedding\n",
    "        \n",
    "        W_in_pos = F.softplus(self.conv_in.weight)  # enforce positivity\n",
    "        \n",
    "        x_term = F.conv2d(x_in, W_in_pos, bias=None, stride=self.conv_in.stride, padding=self.conv_in.padding)\n",
    "        \n",
    "        if self.use_hidden:\n",
    "            z_term = self.conv_hidden(z_hidden)\n",
    "            out = x_term + z_term\n",
    "        else:\n",
    "            out = x_term\n",
    "        \n",
    "        # Apply FiLM conditioning\n",
    "        scale = self.film_scale(cond_emb).unsqueeze(-1).unsqueeze(-1)  # (B,C_out,1,1)\n",
    "        shift = self.film_shift(cond_emb).unsqueeze(-1).unsqueeze(-1)\n",
    "        out = out * scale + shift\n",
    "        \n",
    "        return F.relu(out)\n",
    "\n",
    "\n",
    "class ConvConditionalICNN(nn.Module):\n",
    "    def __init__(self, in_channels=3, cond_dim=10, time_dim=1, hidden_channels=[32,64,64,32]):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.cond_dim = cond_dim\n",
    "        self.time_dim = time_dim\n",
    "        \n",
    "        # Condition + time embedding to fixed dim\n",
    "        self.cond_time_embed = nn.Sequential(\n",
    "            nn.Linear(cond_dim + time_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        layers = []\n",
    "        last_channels = in_channels\n",
    "        for i, h_ch in enumerate(hidden_channels):\n",
    "            layers.append(ConvICNNLayer(last_channels, h_ch, use_hidden=(i>0)))\n",
    "            last_channels = h_ch\n",
    "        self.conv_layers = nn.ModuleList(layers)\n",
    "        \n",
    "        self.final_conv = nn.Conv2d(last_channels, 1, kernel_size=1)  # output scalar potential per pixel\n",
    "    \n",
    "    def forward(self, x, c, t):\n",
    "        # x: (B, C, H, W) image input, requires_grad=True\n",
    "        # c: (B, cond_dim) conditioning\n",
    "        # t: (B, time_dim) time scalar\n",
    "        cond_time = torch.cat([c, t], dim=-1)\n",
    "        cond_emb = self.cond_time_embed(cond_time)  # (B, 128)\n",
    "        \n",
    "        z = None\n",
    "        out = x\n",
    "        for i, layer in enumerate(self.conv_layers):\n",
    "            out = layer(out, z, cond_emb)\n",
    "            z = out\n",
    "        \n",
    "        potential = self.final_conv(out).squeeze(1)  # (B, H, W) scalar potential per pixel\n",
    "        \n",
    "        # To get scalar potential per image: sum or mean spatial dims\n",
    "        scalar_potential = potential.view(potential.size(0), -1).mean(dim=1)  \n",
    "        return scalar_potential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "08262292-6d25-4b27-83d5-c150eaee623e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def velocity_field_image(phi_model, x, c, t):\n",
    "    x = x.requires_grad_(True)  # (B,C,H,W)\n",
    "    potential = phi_model(x, c, t)  # (B,) scalar per image\n",
    "    \n",
    "    grad_outputs = torch.ones_like(potential)\n",
    "    v = autograd.grad(outputs=potential,\n",
    "                      inputs=x,\n",
    "                      grad_outputs=grad_outputs,\n",
    "                      create_graph=True,\n",
    "                      retain_graph=True,\n",
    "                      only_inputs=True)[0]  # (B,C,H,W)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "864a8a05-e88a-45a6-bfe9-018de7a04c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def flow_matching_loss_image(phi_model, x_t, c, t, v_star):\n",
    "#     v_theta = velocity_field_image(phi_model, x_t, c, t)\n",
    "#     loss = ((v_theta - v_star)**2).mean()  # MSE over all pixels, channels, batch\n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "d44d31e6-e3b6-41ac-8ec2-07e59628dcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi = ConvConditionalICNN(in_channels = 1)\n",
    "velocity_field_image(phi, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ba5c60-c503-4e50-9d1a-be9d662e371a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "model = FNOVelocityField(in_channels=1, modes1=16, modes2=16)\n",
    "model = model.cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "k = 0\n",
    "for step in trange(15):\n",
    "    for x, y in tqdm(train_loader, leave = False):\n",
    "        x1 = x.cuda()\n",
    "        x0 = torch.randn_like(x1).cuda()\n",
    "        loss = flow_matching_loss_image(model, x0, x1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        k += 1\n",
    "        if k % 500 == 0: \n",
    "            print(loss.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2a977e-aabb-497f-8a06-ccf0211e1570",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abc80d5-a2e0-4758-a4fa-2fb75cb8c0f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53afd9f0-a1f7-4830-a4a7-9492bdb8458a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f7ca0c-006f-45d8-9843-a28ed591d1ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd7a7cb-965e-4b3a-a765-fd1f5be88962",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9243bd34-5191-4e02-8889-6897d0c43e93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb024860-6224-4d4a-a16d-9ca72f5ad4d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe46318-4b44-4dbd-a8b9-973b2735bc76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "f2d187e0-256b-443d-8b62-c6d8f758d694",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectralVelocityField(nn.Module):\n",
    "    def __init__(self, noise_dim, height, width, channels=1, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.channels = channels\n",
    "        \n",
    "        # Output needs to be real+imaginary parts for each Fourier coefficient\n",
    "        output_dim = 2 * channels * height * width\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(noise_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        z: [B, noise_dim]  -- white noise latent vector\n",
    "        Returns:\n",
    "            velocity_field: [B, C, H, W] in spatial domain\n",
    "        \"\"\"\n",
    "        B = z.size(0)\n",
    "        \n",
    "        coeffs = self.mlp(z)  # [B, 2*C*H*W]\n",
    "        coeffs = coeffs.view(B, self.channels, 2, self.height, self.width)\n",
    "        \n",
    "        # real and imaginary parts\n",
    "        real = coeffs[:, :, 0, :, :]\n",
    "        imag = coeffs[:, :, 1, :, :]\n",
    "        \n",
    "        # create complex tensor for Fourier domain\n",
    "        complex_coeffs = torch.complex(real, imag)\n",
    "        \n",
    "        # inverse FFT to get spatial domain velocity\n",
    "        velocity_field = torch.fft.ifft2(complex_coeffs, norm=\"ortho\").real\n",
    "        \n",
    "        return velocity_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b83ded5-fa6d-4f4b-8842-069b78b11560",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "model = FNOVelocityField(in_channels=1, modes1=16, modes2=16)\n",
    "model = model.cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "k = 0\n",
    "for step in trange(5):\n",
    "    for x, y in tqdm(train_loader, leave = False):\n",
    "        x1 = x.cuda()\n",
    "        x0 = torch.randn_like(x1).cuda()\n",
    "        loss = conv_flow_matching_loss(model, x0, x1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        k += 1\n",
    "        if k % 500 == 0: \n",
    "            print(loss.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ea5b379a-de2c-43eb-a107-e2878e031450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr = 1e-3\n",
    "# model = ConvUNetVelocityField(in_channels=1, base_channels=64, time_embed_dim=16, time_mlp_dim=128, depths=(1,1,2))\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# for step in trange(1):\n",
    "#     for x, y in tqdm(train_loader, leave = False):\n",
    "#       x1 = x\n",
    "#       x0 = torch.randn_like(x1)\n",
    "#       loss, logs = CGM_loss(model, x0, x1, sym_weight=1e-1, eig_weight=1e-1)\n",
    "#       loss.backward()\n",
    "#       optimizer.step()\n",
    "#       optimizer.zero_grad()\n",
    "#       add_sgmcmc_noise(model, lr=lr, noise_scale=1.0)\n",
    "\n",
    "# #   if step % 100 == 0:\n",
    "# #     print(f\"[{step}] Loss: {loss.item():.4f} | FM: {logs['fm_loss']:.4f} | Reg: {logs['reg_penalty']:.4f}\")\n",
    "\n",
    "# # print(f\"[{step}] Loss: {loss.item():.4f} | FM: {logs['fm_loss']:.4f} | Reg: {logs['reg_penalty']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747b063b-0e8b-4fb3-965d-71767b272728",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "c2309aff-0216-44ec-9e02-081527dae449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.fft\n",
    "\n",
    "class LowFreqSpectralVelocityField(nn.Module):\n",
    "    def __init__(self, img_shape, time_embed_dim=16, num_modes=(8, 8), hidden_dim=128):\n",
    "        \"\"\"\n",
    "        img_shape: (C, H, W)   -- shape of the output velocity field\n",
    "        time_embed_dim: int    -- size of time embedding\n",
    "        num_modes: (mH, mW)    -- number of retained Fourier modes in each spatial dimension\n",
    "        hidden_dim: int        -- hidden layer size for MLP\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.C, self.H, self.W = img_shape\n",
    "        self.mH, self.mW = num_modes\n",
    "        self.time_embed_dim = time_embed_dim\n",
    "\n",
    "        # Precompute frequencies for time embedding\n",
    "        freqs = torch.linspace(1.0, 10.0, time_embed_dim)\n",
    "        self.register_buffer(\"freqs\", freqs)\n",
    "\n",
    "        # MLP outputs compact spectral coefficients: \n",
    "        # (C, mH, mW//2+1) complex coefficients → real+imag parts separately\n",
    "        coeffs_dim = self.C * self.mH * (self.mW//2 + 1) * 2  # 2 for real+imag\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(time_embed_dim + self.C, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, coeffs_dim)\n",
    "        )\n",
    "\n",
    "    def time_embed(self, t):\n",
    "        \"\"\"t: [B, 1] → [B, time_embed_dim]\"\"\"\n",
    "        return torch.sin(t * self.freqs)\n",
    "\n",
    "    def forward(self, z, t):\n",
    "        \"\"\"\n",
    "        z: [B, C] white noise vector (per channel amplitude seed)\n",
    "        t: [B, 1] scalar time\n",
    "        Returns: [B, C, H, W] velocity field\n",
    "        \"\"\"\n",
    "        B = z.shape[0]\n",
    "        t_emb = self.time_embed(t)              # [B, time_embed_dim]\n",
    "\n",
    "        print(z.shape, t_emb.shape)\n",
    "        z = image_to_compact_spectrum(z, (self.mH, self.mW))\n",
    "        inp = torch.cat([z, t_emb], dim=-1)     # [B, C + time_embed_dim]\n",
    "\n",
    "        # Get spectral coefficients (real + imag flattened)\n",
    "        coeffs = self.mlp(inp)                  # [B, coeffs_dim]\n",
    "        coeffs = coeffs.view(B, self.C, self.mH, self.mW//2 + 1, 2)\n",
    "\n",
    "        # Pack into complex tensor for irfft\n",
    "        coeffs_complex = torch.complex(coeffs[..., 0], coeffs[..., 1])  # [B, C, mH, mW//2+1]\n",
    "\n",
    "        # Zero-pad to full H, W spectrum\n",
    "        full_spec = torch.zeros(B, self.C, self.H, self.W//2 + 1, dtype=torch.complex64, device=z.device)\n",
    "        full_spec[:, :, :self.mH, :self.mW//2 + 1] = coeffs_complex\n",
    "\n",
    "        # Inverse real FFT to spatial domain\n",
    "        vel_field = torch.fft.irfft2(full_spec, s=(self.H, self.W))  # [B, C, H, W]\n",
    "\n",
    "        return vel_field\n",
    "\n",
    "\n",
    "def image_to_compact_spectrum(x, num_modes):\n",
    "    \"\"\"\n",
    "    Convert image tensor to compact Fourier coefficients.\n",
    "    x: [B, C, H, W]\n",
    "    num_modes: (mH, mW)\n",
    "    Returns: [B, C, mH, mW//2+1, 2] real+imag\n",
    "    \"\"\"\n",
    "    b, c = x.shape[0:2]\n",
    "    mH, mW = num_modes\n",
    "    spec = torch.fft.rfft2(x)  # [B, C, H, W//2+1], complex\n",
    "    compact = spec[:, :, :mH, :mW//2 + 1]\n",
    "    return torch.stack([compact.real, compact.imag], dim=-1).reshape(b, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "9d43207f-fa89-48fc-8b7a-3d0e39b5821c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_flow_matching_loss(model, x0, x1):\n",
    "    \"\"\"\n",
    "    x0, x1: [B, d]   -- samples from conditional marginal\n",
    "    y:     [B, c]   -- conditioning variable\n",
    "    \"\"\"\n",
    "    n, c, h, w = x0.shape\n",
    "    t = torch.rand(n, 1, device=x0.device)\n",
    "    t_conv = t[:,:,None,None]\n",
    "    x_t = (1 - t_conv) * x0 + t_conv * x1\n",
    "    \n",
    "    v_target = x1 - x0\n",
    "    # x_t = torch.fft.fft2(x_t, norm = 'ortho')\n",
    "    # print(x_t.shape)\n",
    "    v_pred = model(x_t, t)\n",
    "    return F.mse_loss(v_pred, v_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "7a51ce1b-a793-4aae-b209-0a4949a7c2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = x.cuda()\n",
    "x0 = torch.randn_like(x1).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "8f8a945f-7eb3-471b-8752-59feab65d0f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 48])"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_to_compact_spectrum(x1 - x0, (6, 6)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "35ae9f60-2403-48e2-b201-0e73f6047906",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LowFreqSpectralVelocityField(img_shape=(1, 28, 28), time_embed_dim=16, num_modes=(8, 8), hidden_dim=128)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "97dba9db-60eb-4853-96cb-975961b5c3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28]) torch.Size([64, 16])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (64x96 and 17x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[273], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mconv_flow_matching_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[269], line 14\u001b[0m, in \u001b[0;36mconv_flow_matching_loss\u001b[0;34m(model, x0, x1)\u001b[0m\n\u001b[1;32m     11\u001b[0m v_target \u001b[38;5;241m=\u001b[39m x1 \u001b[38;5;241m-\u001b[39m x0\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# x_t = torch.fft.fft2(x_t, norm = 'ortho')\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# print(x_t.shape)\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m v_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mmse_loss(v_pred, v_target)\n",
      "File \u001b[0;32m~/climate/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/climate/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[268], line 52\u001b[0m, in \u001b[0;36mLowFreqSpectralVelocityField.forward\u001b[0;34m(self, z, t)\u001b[0m\n\u001b[1;32m     49\u001b[0m inp \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([z, t_emb], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)     \u001b[38;5;66;03m# [B, C + time_embed_dim]\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Get spectral coefficients (real + imag flattened)\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m coeffs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m)\u001b[49m                  \u001b[38;5;66;03m# [B, coeffs_dim]\u001b[39;00m\n\u001b[1;32m     53\u001b[0m coeffs \u001b[38;5;241m=\u001b[39m coeffs\u001b[38;5;241m.\u001b[39mview(B, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mC, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmH, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmW\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Pack into complex tensor for irfft\u001b[39;00m\n",
      "File \u001b[0;32m~/climate/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/climate/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/climate/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/climate/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/climate/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/climate/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (64x96 and 17x128)"
     ]
    }
   ],
   "source": [
    "conv_flow_matching_loss(model, x0, x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc77f9a-e20c-448c-9457-8f290aea9deb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062096fd-8cab-48bb-9dad-f868cca291e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "fd7b07c4-bcf9-4beb-9d66-ac6c38b4c041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c9871e81aa04385b83d1ee0bdbfee11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64e68a36d6af4510a4327f0787ddb7cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 4 and 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[228], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m x1 \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     15\u001b[0m x0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn_like(x1)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m---> 16\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mconv_flow_matching_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[227], line 14\u001b[0m, in \u001b[0;36mconv_flow_matching_loss\u001b[0;34m(model, x0, x1)\u001b[0m\n\u001b[1;32m     12\u001b[0m x_t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfft\u001b[38;5;241m.\u001b[39mfft2(x_t, norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mortho\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(x_t\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 14\u001b[0m v_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mmse_loss(v_pred, v_target)\n",
      "File \u001b[0;32m~/climate/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/climate/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[213], line 76\u001b[0m, in \u001b[0;36mLowFreqSpectralGeneratorHermitian.forward\u001b[0;34m(self, z, t)\u001b[0m\n\u001b[1;32m     73\u001b[0m t_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_mlp(t_sin)  \u001b[38;5;66;03m# [B, time_mlp_dim]\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# --- main MLP input ---\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m mlp_in \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_emb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [B, latent_dim + time_mlp_dim]\u001b[39;00m\n\u001b[1;32m     77\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(mlp_in)  \u001b[38;5;66;03m# [B, out_dim]\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# reshape into complex low-frequency block\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 4 and 2"
     ]
    }
   ],
   "source": [
    "lr = 1e-3\n",
    "# model = FNOVelocityField(in_channels=1, modes1=16, modes2=16)\n",
    "\n",
    "latent_dim = 128\n",
    "model = LowFreqSpectralGeneratorHermitian(latent_dim=latent_dim, img_size=(28, 28),\n",
    "                                              channels=1, modes=(6, 6), hidden_dim=512)\n",
    "\n",
    "model = model.cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "k = 0\n",
    "for step in trange(5):\n",
    "    for x, y in tqdm(train_loader, leave = False):\n",
    "        x1 = x.cuda()\n",
    "        x0 = torch.randn_like(x1).cuda()\n",
    "        loss = conv_flow_matching_loss(model, x0, x1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        k += 1\n",
    "        if k % 500 == 0: \n",
    "            print(loss.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4475cba8-41d5-483f-bfc3-96c77070761d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b06971-54d9-4fc0-b41d-6ebd6793b627",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "f4fe13a0-f1ed-4b62-928a-1b3be9f68712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 124]) 180 64\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (64x124 and 244x512)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[291], line 211\u001b[0m\n\u001b[1;32m    207\u001b[0m model \u001b[38;5;241m=\u001b[39m SpectralVelocityEstimator(img_shape\u001b[38;5;241m=\u001b[39m(C, H, W), modes\u001b[38;5;241m=\u001b[39mmodes,\n\u001b[1;32m    208\u001b[0m                                   time_embed_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, time_mlp_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, hidden_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\n\u001b[1;32m    210\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_cfm_spectral\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5e-4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[291], line 176\u001b[0m, in \u001b[0;36mtrain_cfm_spectral\u001b[0;34m(model, dataloader, device, epochs, lr, s_fn, s_dot_fn)\u001b[0m\n\u001b[1;32m    173\u001b[0m v_star \u001b[38;5;241m=\u001b[39m s_dot\u001b[38;5;241m.\u001b[39mview(B, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m (x1 \u001b[38;5;241m-\u001b[39m x0)    \u001b[38;5;66;03m# [B,C,H,W]\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m# model predicts v_hat (spatial) from x_t and t (via spectral internals)\u001b[39;00m\n\u001b[0;32m--> 176\u001b[0m v_hat, pred_packed \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m loss \u001b[38;5;241m=\u001b[39m mse(v_hat, v_star)\n\u001b[1;32m    180\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/climate/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/climate/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[291], line 122\u001b[0m, in \u001b[0;36mSpectralVelocityEstimator.forward\u001b[0;34m(self, x_t_image, t)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28mprint\u001b[39m(mlp_in\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_spec_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_mlp_dim)\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# 3) Predict compact spectral velocity coeffs (real+imag)\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmlp_in\u001b[49m\u001b[43m)\u001b[49m         \u001b[38;5;66;03m# [B, in_spec_dim]\u001b[39;00m\n\u001b[1;32m    123\u001b[0m out_packed \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mview(B, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mC, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmH, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmW_half, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# [B,C,mH,mW_half,2]\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# 4) build full half-spectrum tensor for irfft2 (vectorized)\u001b[39;00m\n",
      "File \u001b[0;32m~/climate/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/climate/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/climate/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/climate/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/climate/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/climate/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (64x124 and 244x512)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.fft\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Helpers: compact-spectrum <-> image\n",
    "# -------------------------\n",
    "def image_to_compact_spectrum(x, mH, mW):\n",
    "    \"\"\"\n",
    "    x: [B, C, H, W] float real image\n",
    "    returns: real+imag packed tensor [B, C, mH, mW_half, 2]\n",
    "    where mW_half = W//2 + 1\n",
    "    \"\"\"\n",
    "    B, C, H, W = x.shape\n",
    "    assert mH <= H and mW <= W, \"modes must be <= image dims\"\n",
    "    spec = torch.fft.rfft2(x)                      # [B, C, H, W//2 + 1] complex\n",
    "    mW_half = W // 2 + 1\n",
    "    compact = spec[:, :, :mH, : (mW_half if mW >= mW_half else (mW//2 + 1))]\n",
    "    # pack real+imag\n",
    "    packed = torch.stack([compact.real, compact.imag], dim=-1)  # [B, C, mH, mW_half, 2]\n",
    "    return packed\n",
    "\n",
    "\n",
    "def compact_spectrum_to_full_spec(packed, H, W):\n",
    "    \"\"\"\n",
    "    packed: [B, C, mH, mW_half, 2] real+imag\n",
    "    returns: full compact half-spectrum shaped for irfft2:\n",
    "             [B, C, H, W//2 + 1] complex\n",
    "    places the low-frequency block in the top-left corner and zeros the rest.\n",
    "    \"\"\"\n",
    "    B, C, mH, mW_half, _ = packed.shape\n",
    "    device = packed.device\n",
    "    full_half = torch.zeros(B, C, H, W // 2 + 1, dtype=torch.cfloat, device=device)\n",
    "    complex_block = torch.complex(packed[..., 0], packed[..., 1])  # [B,C,mH,mW_half]\n",
    "    full_half[:, :, :mH, :mW_half] = complex_block\n",
    "    return full_half\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Time embed helper (sinusoidal freqs -> small MLP)\n",
    "# -------------------------\n",
    "class TimeEmbed(nn.Module):\n",
    "    def __init__(self, time_embed_dim=16, time_mlp_dim=64):\n",
    "        super().__init__()\n",
    "        freqs = torch.linspace(1.0, 10.0, time_embed_dim)\n",
    "        self.register_buffer(\"freqs\", freqs)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(time_embed_dim, time_mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(time_mlp_dim, time_mlp_dim),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, t):\n",
    "        # t: [B,1] or [B]\n",
    "        if t.dim() == 1:\n",
    "            t = t.unsqueeze(-1)\n",
    "        t_sin = torch.sin(t * self.freqs.to(t.device))   # [B, time_embed_dim]\n",
    "        return self.mlp(t_sin)                            # [B, time_mlp_dim]\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Spectral velocity estimator\n",
    "# -------------------------\n",
    "class SpectralVelocityEstimator(nn.Module):\n",
    "    \"\"\"\n",
    "    Input: compact spectral coefficients of x_t (real+imag packed) + time t\n",
    "    Output: compact spectral coefficients of velocity v_hat (real+imag packed)\n",
    "    We compute loss in spatial domain: v_hat_spatial = irfft2(pred_compact_full)\n",
    "    \"\"\"\n",
    "    def __init__(self, img_shape, modes, time_embed_dim=16, time_mlp_dim=64, hidden_dim=512):\n",
    "        \"\"\"\n",
    "        img_shape: (C, H, W)\n",
    "        modes: (mH, mW) retained low-frequency modes\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.C, self.H, self.W = img_shape\n",
    "        self.mH, self.mW = modes\n",
    "        self.mW_half = self.W // 2 + 1     # compact half-spectrum width\n",
    "\n",
    "        # dims for packed compact spectrum as input\n",
    "        self.in_spec_dim = self.C * self.mH * self.mW_half * 2   # real+imag\n",
    "        self.time_mlp_dim = time_mlp_dim\n",
    "\n",
    "        # time embed\n",
    "        self.time_embed = TimeEmbed(time_embed_dim=time_embed_dim, time_mlp_dim=time_mlp_dim)\n",
    "\n",
    "        # MLP: input = flattened compact-spec + time embedding\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.in_spec_dim + self.time_mlp_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, self.in_spec_dim)   # predict real+imag for velocity compact block\n",
    "        )\n",
    "\n",
    "    def forward(self, x_t_image, t):\n",
    "        \"\"\"\n",
    "        x_t_image: [B, C, H, W] real image (spatial)\n",
    "        t: [B,1] time\n",
    "        returns: v_hat_spatial: [B, C, H, W] (real spatial velocity predicted)\n",
    "                 and optionally the predicted compact spectrum (packed)\n",
    "        \"\"\"\n",
    "        B = x_t_image.shape[0]\n",
    "        device = x_t_image.device\n",
    "\n",
    "        # 1) Convert image -> packed compact spectrum\n",
    "        packed = image_to_compact_spectrum(x_t_image, self.mH, self.mW)\n",
    "        # packed: [B, C, mH, mW_half, 2]\n",
    "\n",
    "        # 2) Flatten and concat time embedding\n",
    "        flat = packed.reshape(B, -1)   # [B, in_spec_dim]\n",
    "        t_emb = self.time_embed(t)     # [B, time_mlp_dim]\n",
    "        mlp_in = torch.cat([flat, t_emb], dim=-1)  # [B, in_spec_dim + time_mlp_dim]\n",
    "\n",
    "        print(mlp_in.shape, self.in_spec_dim, self.time_mlp_dim)\n",
    "\n",
    "        # 3) Predict compact spectral velocity coeffs (real+imag)\n",
    "        out = self.mlp(mlp_in)         # [B, in_spec_dim]\n",
    "        out_packed = out.view(B, self.C, self.mH, self.mW_half, 2)  # [B,C,mH,mW_half,2]\n",
    "\n",
    "        # 4) build full half-spectrum tensor for irfft2 (vectorized)\n",
    "        full_half = compact_spectrum_to_full_spec(out_packed, self.H, self.W)  # [B,C,H,W//2+1] complex\n",
    "\n",
    "        # 5) inverse rfft -> spatial velocity\n",
    "        v_hat = torch.fft.irfft2(full_half, s=(self.H, self.W), norm=\"ortho\")  # [B,C,H,W] real\n",
    "        return v_hat, out_packed\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# CFM training loop (simple OT interpolation)\n",
    "# -------------------------\n",
    "def train_cfm_spectral(\n",
    "    model,\n",
    "    dataloader,\n",
    "    device,\n",
    "    epochs=10,\n",
    "    lr=1e-4,\n",
    "    s_fn=lambda t: t,          # interpolation schedule s(t)\n",
    "    s_dot_fn=lambda t: torch.ones_like(t)  # derivative s'(t)\n",
    "):\n",
    "    \"\"\"\n",
    "    model: SpectralVelocityEstimator\n",
    "    dataloader: yields [B,C,H,W] images x0\n",
    "    We'll set x1 = Gaussian white noise images (OT target) for simplicity.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    mse = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (x0, _) in enumerate(dataloader):\n",
    "            x0 = x0.to(device)                             # [B,C,H,W]\n",
    "            B = x0.shape[0]\n",
    "\n",
    "            # sample x1 from simple Gaussian prior (white noise in pixel space)\n",
    "            x1 = torch.randn_like(x0)                      # [B,C,H,W]\n",
    "\n",
    "            # sample t uniformly in (0,1)\n",
    "            t = torch.rand(B, 1, device=device)            # [B,1]\n",
    "            s = s_fn(t)                                    # [B,1]\n",
    "            s_dot = s_dot_fn(t)                            # [B,1]\n",
    "            \n",
    "            # compute OT-like linear interpolation x_t\n",
    "            s_conv = s[:,:,None,None]\n",
    "            x_t = (1.0 - s_conv) * x0 + s_conv * x1                   # [B,C,H,W]\n",
    "\n",
    "            # target velocity (time derivative of interpolation)\n",
    "            # v* = s'(t) * (x1 - x0)\n",
    "            v_star = s_dot.view(B, 1, 1, 1) * (x1 - x0)    # [B,C,H,W]\n",
    "\n",
    "            # model predicts v_hat (spatial) from x_t and t (via spectral internals)\n",
    "            v_hat, pred_packed = model(x_t, t)\n",
    "\n",
    "            loss = mse(v_hat, v_star)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f\"Epoch {epoch} Batch {batch_idx} Loss {loss.item():.4e}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Minimal runnable example using MNIST\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Data: MNIST scaled to [-1,1]\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    ds = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "    loader = DataLoader(ds, batch_size=64, shuffle=True, drop_last=True)\n",
    "\n",
    "    # Model config\n",
    "    C, H, W = 1, 28, 28\n",
    "    modes = (6, 8)   # mH, mW (mW should be <= W)\n",
    "    model = SpectralVelocityEstimator(img_shape=(C, H, W), modes=modes,\n",
    "                                      time_embed_dim=16, time_mlp_dim=64, hidden_dim=512)\n",
    "\n",
    "    # Train\n",
    "    model = train_cfm_spectral(model, loader, device, epochs=3, lr=5e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeaba3b7-9ea2-4cfa-92d0-109c603f000c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac2df83-aa9e-4ccb-b2fe-a4ec966d67f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
